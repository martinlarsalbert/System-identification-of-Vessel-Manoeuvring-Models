{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd9ec962-8112-473a-9b54-4051d985ab8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# manoeuvring model Bias-variance tradeoff\n",
    "\n",
    "The accelerations ($y$) obtained from the model tests are generated by the unknown real model $f(x)$ and the irreducible error $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$ .\n",
    "\n",
    "$$\n",
    "y = f(x) + \\epsilon\n",
    "$$ (eq_y)\n",
    "\n",
    "In the manoeuvring models $\\hat{f}(x)$ (predictor) approximates the true model $f(x)$ \n",
    "\n",
    "$$\n",
    "y = \\hat{f}(x) + \\epsilon_G\n",
    "$$ (eq_y)\n",
    "\n",
    "The predictor is fitted on a dataset $D$ (the model tests).\n",
    "\n",
    "$$\n",
    "D = \\{(x_1,y_1)..(x_n,y_n) \\}\n",
    "$$ (eq_D)\n",
    "\n",
    "$$\n",
    "y = \\hat{f}_D(x) + \\epsilon_G\n",
    "$$ (eq_y)\n",
    "\n",
    "For the predictor to be usefull it must perform well on unseen data so that it generalizes to $x$ values outside the training dataset $D$. The predictor will then have a generalization error which can be decomposed in bias error, variance and irreducible error. The bias error describes how well the predictor $\\hat{f}(x)$ approximates the real model ${f}(x)$. The variance describes the robustness of the predictor, how much it changes with different training datasets. The irreducible error origins from inherent noise in the problem itself and is an error term that will always be present.\n",
    "\n",
    "A simple predictor simplifies the real model too much giving a high bias error. A simple model is not so sensitive to the training data and are therefore robust and has a similar performance regardless of the training dataset which giving a low variance. \n",
    "A more complex model acts more like the real model giving lower bias but may also have a high variance if the model generalizes very badly to data outside the training dataset.\n",
    "Very simple models underfitts the training data and the too complex models overfitts it.  \n",
    "The goal is to create a predictor with both low bias error and low variance which usually is a tradeoff between the two.\n",
    "\n",
    "The expected error of $\\hat{f}(x)$ on unseen sample $x$ can be decomposed as:\n",
    "\n",
    "$$\n",
    "\\mathop{\\mathbb{E_D}} \\left[ (y-\\hat{f}_D(x))^2 \\right] = MSE = \\left( Bias_D[\\hat{f}(x)]\\right)^2 +Var_D[\\hat{f}(x)] + \\sigma^2\n",
    "$$ (eq_bias_variance)\n",
    "\n",
    "$$\n",
    "Bias_D[\\hat{f}(x)] = \\mathop{\\mathbb{E_D}} \\left[\\hat{f}_D(x) \\right] - f(x)\n",
    "$$ (eq_bias)\n",
    "\n",
    "$$\n",
    "Var_D[\\hat{f}(x)] = \\mathop{\\mathbb{E_D}} \\left[ (\\mathop{\\mathbb{E_D}} [\\hat{f}_D(x)] - \\hat{f}_D(x))^2 \\right] \n",
    "$$ (eq_variance)\n",
    "\n",
    "\n",
    "Since the real model $f(x)$ is not known the expected bias is calculated from the observed values $y$\n",
    "\n",
    "$$\n",
    "Bias_D[\\hat{f}(x)] = \\mathop{\\mathbb{E_D}} \\left[\\hat{f}_D(x) \\right] - y\n",
    "$$ (eq_bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c03299-1848-4a97-b6e6-e3dd61272a3d",
   "metadata": {},
   "source": [
    "To find the best bias-variance tradeoff the data is first divided into a training set and a test set. The training set is further divided into sub training sets. Predictions of values in the test set is then performed with models fitted to the sub training sets. The bias and variance can then be calculated for each manoeuvring model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bf1c1a-7c2a-4d36-b3eb-7a6f1c41666e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# %load imports.py\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_kedro\n",
    "%config Completer.use_jedi = False  ## (To fix autocomplete)\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "from vessel_manoeuvring_models.models.vmm import ModelSimulator\n",
    "import matplotlib.pyplot as plt\n",
    "from vessel_manoeuvring_models.visualization.plot import track_plots, plot, captive_plot\n",
    "import kedro\n",
    "import numpy as np\n",
    "import os.path\n",
    "import anyconfig\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams[\"figure.figsize\"] = (15,4)\n",
    "\n",
    "from myst_nb import glue\n",
    "from vessel_manoeuvring_models.symbols import *\n",
    "import vessel_manoeuvring_models.symbols as symbols\n",
    "from vessel_manoeuvring_models.system_equations import *\n",
    "\n",
    "from IPython.display import display, Math, Latex, Markdown\n",
    "from sympy.physics.vector.printing import vpprint, vlatex\n",
    "\n",
    "from vessel_manoeuvring_models.parameters import df_parameters\n",
    "p = df_parameters[\"symbol\"]\n",
    "\n",
    "# Read configs:\n",
    "conf_path = os.path.join(\"../../conf/base/\")\n",
    "runs_globals_path = os.path.join(\n",
    "    conf_path,\n",
    "    \"runs_globals.yml\",\n",
    ")\n",
    "\n",
    "runs_globals = anyconfig.load(runs_globals_path)\n",
    "model_test_ids = runs_globals[\"model_test_ids\"]\n",
    "\n",
    "join_globals_path = os.path.join(\n",
    "    conf_path,\n",
    "    \"join_globals.yml\",\n",
    ")\n",
    "\n",
    "joins = runs_globals[\"joins\"]\n",
    "join_runs_dict = anyconfig.load(join_globals_path)\n",
    "\n",
    "globals_path = os.path.join(\n",
    "    conf_path,\n",
    "    \"globals.yml\",\n",
    ")\n",
    "global_variables = anyconfig.load(globals_path)\n",
    "\n",
    "\n",
    "\n",
    "vmm_names = global_variables[\"vmms\"]\n",
    "only_joined = global_variables[\n",
    "    \"only_joined\"\n",
    "]  # (regress/predict with only models from joined runs)\n",
    "\n",
    "from vessel_manoeuvring_models.visualization.plot import plot, track_plots\n",
    "ship_data = catalog.load(\"ship_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9c6194-3162-4ef7-bddd-eea0e30e09c1",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from wPCC_pipeline.pipelines.motion_regression.nodes import fit_motions\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "from vessel_manoeuvring_models.bias_variance_tradeoff import (train_test_split, \n",
    "                                        train_test_split_run, \n",
    "                                        train_test_split_exteme, \n",
    "                                        train_predict, \n",
    "                                        pivot_mean, \n",
    "                                        variances,\n",
    "                                        expected,\n",
    "                                        errors,\n",
    "                                        error_bars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd1384b-a927-4c80-8d48-4a454450d03e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "join = \"joined\"\n",
    "data = catalog.load(f\"{ join }.data_ek_smooth\")\n",
    "\n",
    "added_masses = catalog.load(\"added_masses\")\n",
    "ship_data = catalog.load(\"ship_data\")\n",
    "exclude_parameters = catalog.load(\"params:motion_regression.exclude_parameters\")\n",
    "\n",
    "runs_meta_data = catalog.load(\"runs_meta_data\")\n",
    "runs_meta_data.sort_values(by='description', inplace=True)\n",
    "\n",
    "vmms = {}\n",
    "for vmm_name in vmm_names:\n",
    "    vmms[vmm_name] = catalog.load(f\"{vmm_name}\")\n",
    "    \n",
    "regressions = {}\n",
    "for vmm_name in vmm_names:\n",
    "    regressions[vmm_name] = catalog.load(f\"{ vmm_name }.motion_regression.joined.regression\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a199a51-4c62-4d41-9122-4ce47a640226",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "data_ = data.copy()\n",
    "data_['beta'] = -np.arctan2(data_['v'],data['u'])\n",
    "data_['beta_deg'] = np.rad2deg(data_['beta'])\n",
    "data_['r_deg'] = np.rad2deg(data_['r'])\n",
    "data_['delta_deg'] = np.rad2deg(data_['delta'])\n",
    "\n",
    "mask = runs_meta_data.index.isin(data_['id'].unique())\n",
    "runs_meta_data_selected = runs_meta_data.loc[mask].copy()\n",
    "mask = runs_meta_data_selected.duplicated(subset='description', keep='last')\n",
    "runs_meta_data_selected = runs_meta_data_selected.loc[~mask]\n",
    "data_groups = data_.groupby(by='id')\n",
    "\n",
    "for id, meta_data in runs_meta_data_selected.iterrows():\n",
    "    \n",
    "    if not id in data_groups.groups.keys():\n",
    "        continue\n",
    "    \n",
    "    df_ = data_groups.get_group(id)\n",
    "    description = runs_meta_data.loc[id]['description']\n",
    "    df_.plot(x='beta_deg', y='r_deg', ax=ax, label=description)\n",
    "    \n",
    "ax.set_ylabel(r'$r$ (yaw rate) $[deg/s]$')\n",
    "ax.set_xlabel(r'$\\beta$ (drift angle) $[deg]$');\n",
    "\n",
    "fig_name = f\"fig_beta_vs_r\"\n",
    "glue(fig_name, fig, display=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d34a25-48c5-476f-977d-63dfb84edb08",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "for id, meta_data in runs_meta_data_selected.iterrows():\n",
    "    \n",
    "    if not id in data_groups.groups.keys():\n",
    "        continue\n",
    "    \n",
    "    df_ = data_groups.get_group(id)\n",
    "    description = runs_meta_data.loc[id]['description']\n",
    "    df_.plot(x='delta_deg', y='u', ax=ax, label=description)\n",
    "    \n",
    "ax.set_ylabel(r'$u$ (speed) $[m/s]$')\n",
    "ax.set_xlabel(r'$\\delta$ (rudder angle) $[deg]$');\n",
    "\n",
    "fig_name = f\"fig_delta_vs_u\"\n",
    "glue(fig_name, fig, display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bdb907-a36f-43f8-8203-1adf7611c33e",
   "metadata": {},
   "source": [
    "Selecting which part of the data that should be in the test set can be done in many ways, depending on how the model should generalize to data outside the training data. The drift angles, yaw rates, rudder angles and speed are shown in {numref}```fig_beta_vs_r``` and {numref}```fig_delta_vs_u```. It can be seen that the Turning circle test is very different from the other tests in terms of: maximum drift and rudder angles as well as the minimum speed. \n",
    "\n",
    "```{glue:figure} fig_beta_vs_r\n",
    ":figwidth: 600px\n",
    ":name: \"fig_beta_vs_r\"\n",
    "Drift angle and yaw rate from all the model tests.\n",
    "```\n",
    "\n",
    "```{glue:figure} fig_delta_vs_u\n",
    ":figwidth: 600px\n",
    ":name: \"fig_delta_vs_u\"\n",
    "Drift angle and yaw rate from all the model tests.\n",
    "```\n",
    "\n",
    "The bias-variance tradeoff will be investigated for three scenarios having the three differen test sets:\n",
    "* Random test set.\n",
    "* Extreme values as test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cb1866-4193-488e-b5e7-4a8426fca0df",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "regression = regressions['vmm_martins_simple']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf1f148-f290-449d-9e00-df7aa6dd4892",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Random test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b391caa9-2bcf-4975-b87a-b71a222660cc",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test, train_data = train_test_split(X=regression.X_N,\n",
    "                                                                y=regression.y_N, \n",
    "                                                                test_ratio=0.01)\n",
    "\n",
    "\n",
    "fig,ax=plt.subplots()\n",
    "y_train.plot(ax=ax, label='train', style='.')\n",
    "y_test.plot(ax=ax, label='test', style='.')\n",
    "ax.set_xlabel('sample')\n",
    "ax.set_ylabel('$y$')\n",
    "ax.legend();\n",
    "\n",
    "fig_name = \"fig_test_split_random\"\n",
    "glue(fig_name, fig, display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3638364-972c-49fa-841d-6466d3dbf967",
   "metadata": {},
   "source": [
    "{numref}```fig_test_split_random``` show how the data (yaw acceleration in this case) from all the model tests have been randomly split into a training set and a test set.\n",
    "\n",
    "\n",
    "```{glue:figure} fig_test_split_random\n",
    ":figwidth: 600px\n",
    ":name: \"fig_test_split_random\"\n",
    "Random train test split for the yaw acceleration data.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad158d7-616c-4961-8e45-444c3487e33e",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def train_predict_vmms_random(dof='N'):\n",
    "\n",
    "    df_sample_predictions = pd.DataFrame()\n",
    "    \n",
    "    for vmm_name, regression in regressions.items():\n",
    "        \n",
    "        X_name = f\"X_{dof}\"\n",
    "        y_name = f\"y_{dof}\"\n",
    "        X = getattr(regression, X_name)\n",
    "        y = getattr(regression, y_name)\n",
    "        \n",
    "        X_train, y_train, X_test, y_test, train_data = train_test_split(X=X,\n",
    "                                                                        y=y, \n",
    "                                                                        test_ratio=0.01)\n",
    "        \n",
    "        df_ = train_predict(train_data, X_test=X_test, y_test=y_test, train_ratio=0.005, N_trainings=50)\n",
    "        df_['vmm'] = vmm_name\n",
    "        df_sample_predictions = df_sample_predictions.append(df_, ignore_index=True)\n",
    "        \n",
    "    df_sample_predictions.sort_values(by=['parameters','x'], inplace=True)\n",
    "    \n",
    "    df_sample_predictions['residual'] = df_sample_predictions['y_hat'] - df_sample_predictions['z']\n",
    "    df_sample_predictions['residual^2'] = df_sample_predictions['residual']**2\n",
    "    return df_sample_predictions,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e797edb-66fd-4074-9d97-61acccc2771e",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for dof in ['X','Y','N']:\n",
    "    \n",
    "    df_sample_predictions_random, y_test = train_predict_vmms_random(dof=dof)\n",
    "    df_errors_random = errors(df_sample_predictions=df_sample_predictions_random, y_test=y_test)\n",
    "    \n",
    "    if dof=='X':\n",
    "        smart_scale = True\n",
    "    else:\n",
    "        smart_scale = False\n",
    "    \n",
    "    fig = error_bars(df_errors_random, smart_scale=smart_scale)\n",
    "    fig_name = f\"fig_error_bars_random_{dof}\"\n",
    "    glue(fig_name, fig, display=False)   \n",
    "    display(df_errors_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8a7c0b-27d9-4a0e-9a2f-db7a5e155e53",
   "metadata": {},
   "source": [
    "The generalization error expressed as the Mean Square Error MSE from prediction on the random test set with many models trained on subsets of the training set for the surge, sway and yaw regressions are shown in {numref}```fig_error_bars_random_X```, {numref}```fig_error_bars_random_X``` and {numref}```fig_error_bars_random_N```.\n",
    "\n",
    "The errors have been decomposed into bias and variance. The manoeuvring models are ordered by their complexity (number of parameters) with the linear model being the simplest and the Abkowitz model being the most complex. It can be seen that the Abkowitz model has a large variance that contributes to the error.\n",
    "\n",
    "```{glue:figure} fig_error_bars_random_X\n",
    ":figwidth: 600px\n",
    ":name: \"fig_error_bars_random_X\"\n",
    "Error bars for surge acceleration regressions with random test set.\n",
    "```\n",
    "\n",
    "```{glue:figure} fig_error_bars_random_Y\n",
    ":figwidth: 600px\n",
    ":name: \"fig_error_bars_random_Y\"\n",
    "Error bars for sway acceleration regressions with random test set.\n",
    "```\n",
    "\n",
    "```{glue:figure} fig_error_bars_random_N\n",
    ":figwidth: 600px\n",
    ":name: \"fig_error_bars_random_N\"\n",
    "Error bars for yaw acceleration regressions with random test set.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a34a9e-5526-4267-9467-aaf2eb4a6c59",
   "metadata": {},
   "source": [
    "## Extreme test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9947755b-e223-43d6-bec0-a7f6cccae45c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test, train_data = train_test_split_exteme(X=regression.X_N, y=regression.y_N, data=data, \n",
    "                                                                          min_ratio=0.05, \n",
    "                                                                          max_ratio=0.05,\n",
    "                                                                          min_keys=['u'],\n",
    "                                                                          max_keys=['v','r','delta'])\n",
    "\n",
    "\n",
    "fig,ax=plt.subplots()\n",
    "\n",
    "y_train.plot(ax=ax, label='train', style='.')\n",
    "y_test.plot(ax=ax, label='test', style='.')\n",
    "\n",
    "index = data.groupby(by='id').get_group(22774).index\n",
    "ax.fill_between(index, y1=regression.y_N.max(), y2=regression.y_N.min(), color='lightgrey', label='Turning Circle')\n",
    "\n",
    "ax.set_xlabel('sample')\n",
    "ax.set_ylabel('$y$')\n",
    "ax.legend();\n",
    "\n",
    "fig_name = \"fig_test_split_extreme\"\n",
    "glue(fig_name, fig, display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0fda8a-2e90-4fb7-bf19-0e642c202b6d",
   "metadata": {},
   "source": [
    "{numref}```fig_test_split_extreme``` show how the data (yaw acceleration in this case) from all the model tests have been split into a training set and a test set. The test set have been selected to contain all the extreme values from the model tests being the data with 5% largest absolute value of $v$, $r$ and $\\delta$ and the 5% smallest absolute value of $u$. This reflects most of the data from the Turning circle test as indicated in {numref}```fig_test_split_extreme```.\n",
    "\n",
    "\n",
    "\n",
    "```{glue:figure} fig_test_split_extreme\n",
    ":figwidth: 600px\n",
    ":name: \"fig_test_split_extreme\"\n",
    "Random train test split for the yaw acceleration data.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39443415-9573-48fd-85ab-9c55e9505a3d",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def train_predict_vmms_extreme(dof='N'):\n",
    "    \n",
    "    df_sample_predictions = pd.DataFrame()\n",
    "    \n",
    "    for vmm_name, regression in regressions.items():\n",
    "        \n",
    "        X_name = f\"X_{dof}\"\n",
    "        y_name = f\"y_{dof}\"\n",
    "        X = getattr(regression, X_name)\n",
    "        y = getattr(regression, y_name)\n",
    "                \n",
    "        X_train, y_train, X_test, y_test, train_data = train_test_split_exteme(X=X,\n",
    "                                                                            y=y,\n",
    "                                                                            data=data,\n",
    "                                                                            min_ratio=0.05, \n",
    "                                                                            max_ratio=0.05,\n",
    "                                                                            min_keys=['u'],\n",
    "                                                                            max_keys=['v','r'])\n",
    "        \n",
    "        df_ = train_predict(train_data, X_test=X_test, y_test=y_test, train_ratio=0.01, N_trainings=100)\n",
    "        df_['vmm'] = vmm_name\n",
    "        df_sample_predictions = df_sample_predictions.append(df_)\n",
    "        \n",
    "    df_sample_predictions.sort_values(by=['parameters','x'], inplace=True)\n",
    "    \n",
    "    df_sample_predictions['residual'] = df_sample_predictions['y_hat'] - df_sample_predictions['z']\n",
    "    df_sample_predictions['residual^2'] = df_sample_predictions['residual']**2\n",
    "    return df_sample_predictions, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eedcf1-d542-4b8f-965d-8e66b463ab79",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for dof in ['X','Y','N']:\n",
    "    \n",
    "    df_sample_predictions_extreme, y_test = train_predict_vmms_extreme(dof=dof)\n",
    "    df_errors_extreme = errors(df_sample_predictions=df_sample_predictions_extreme, y_test=y_test)\n",
    "    \n",
    "    if dof=='X':\n",
    "        smart_scale = True\n",
    "    else:\n",
    "        smart_scale = False\n",
    "    \n",
    "    fig = error_bars(df_errors_extreme, smart_scale=smart_scale)\n",
    "    fig_name = f\"fig_error_bars_extreme_{dof}\"\n",
    "    glue(fig_name, fig, display=False)   \n",
    "    \n",
    "    display(df_errors_extreme)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cca7333-0697-4e60-9b2c-b1b3a0a596b1",
   "metadata": {},
   "source": [
    "The generalization error expressed as the Mean Square Error MSE from prediction on the extreme test set with many models trained on subsets of the training set for the surge, sway and yaw regressions are shown in {numref}```fig_error_bars_extreme_X```, {numref}```fig_error_bars_extreme_X```, {numref}```fig_error_bars_extreme_N```.\n",
    "\n",
    "The trend with high variance for the Abkowitz model, being the most complex model, is even more pronounced when using the extreme test set.\n",
    "\n",
    "```{glue:figure} fig_error_bars_extreme_X\n",
    ":figwidth: 600px\n",
    ":name: \"fig_error_bars_extreme_X\"\n",
    "Error bars for surge acceleration regressions with extreme test set.\n",
    "```\n",
    "\n",
    "```{glue:figure} fig_error_bars_extreme_Y\n",
    ":figwidth: 600px\n",
    ":name: \"fig_error_bars_extreme_Y\"\n",
    "Error bars for sway acceleration regressions with extreme test set.\n",
    "```\n",
    "\n",
    "```{glue:figure} fig_error_bars_extreme_N\n",
    ":figwidth: 600px\n",
    ":name: \"fig_error_bars_extreme_N\"\n",
    "Error bars for yaw acceleration regressions with extreme test set.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08732fa7-71f5-4a22-b7ba-726099579c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0e7724-99a1-4335-84e4-264340de5053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b89e9b-d321-452d-8105-1f147c30c0f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23fe054-ae67-4c49-bcdf-7f28b7e1d152",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c300b4-e42b-432d-9f00-9aec63e67cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "citation-manager": {
   "items": {}
  },
  "kernelspec": {
   "display_name": "wPCC_pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
